# -*- coding: utf-8 -*-
"""002b_language_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_1p85eJh5KP7k5ahnRKSjD2MhdvIskYI
"""

!pip install --upgrade transformers ## Upgraded to 4.39.2 version
!pip install tensorflow ##==2.11.0
!pip install tf-keras
!pip install --upgrade pip
!pip install --upgrade pyarrow
!pip install datasets
!pip install transformers tensorflow datasets
!pip install colab-convert ## comentar en script
!pip install datasets

# Get the list of user's
env_var = os.environ
os.environ['TF_USE_LEGACY_KERAS'] = '1' ### I’ve set up the legacy version

import os
import pickle
import json
import pandas as pd
pd.set_option("display.max_colwidth", None)
import numpy as np

from collections import Counter
from math import ceil
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import PolynomialDecay
from datasets import load_dataset
from transformers import create_optimizer, TFAutoModelForSequenceClassification, DistilBertTokenizer
from transformers import DataCollatorWithPadding, TFDistilBertForSequenceClassification
from transformers import TFRobertaForSequenceClassification, RobertaTokenizer
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

from google.colab import drive ## comentar en script

"""### Loading Affiliation Dictionary"""

drive.mount('/content/drive') ## comentar en script

"""**Cambiar path**"""

base_path = '/content/drive/MyDrive/openalex-institution-parsing/'

# Loading the affiliation (target) vocab
with open(f"{base_path}Crudos/institution_tagger_v2_artifacts/affiliation_vocab_argentina.pkl","rb") as f:
  affiliation_vocab = pickle.load(f)

affiliation_vocab = {int(i):int(j) for i,j in affiliation_vocab.items()}
inverse_affiliation_vocab = {i:j for j,i in affiliation_vocab.items()}
print("inverse_affiliation_vocab")
print(inverse_affiliation_vocab)

with open(f"{base_path}Crudos/institution_tagger_v2_artifacts/affiliation_vocab_argentina.pkl","wb") as f:
    pickle.dump(affiliation_vocab, f)

print(len(affiliation_vocab))

"""### Tokenizing Affiliation String"""

# Loading the standard DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased", return_tensors='tf')

tokenizer

# Using the HuggingFace library to load the dataset
train_dataset = load_dataset("parquet", data_files={'train': f'{base_path}V2/002_Model/training_data/train_data.parquet'})
val_dataset = load_dataset("parquet", data_files={'val': f'{base_path}V2/002_Model/training_data/val_data.parquet'})

train_dataset

print(pd.DataFrame(train_dataset).head())

print(val_dataset)

MAX_LEN = 256

def preprocess_function(examples):
    return tokenizer(examples["processed_text"], truncation=True, padding=True,
                     max_length=MAX_LEN)

train_dataset.map(preprocess_function, batched=False)

# Tokenizing the train dataset
tokenized_train_data = train_dataset.map(preprocess_function, batched=False)

tokenized_train_data

tokenized_train_data.cleanup_cache_files()
tokenized_train_data

# Tokenizing the validation dataset
tokenized_val_data = val_dataset.map(preprocess_function, batched=False)

tokenized_val_data

tokenized_val_data.cleanup_cache_files()

"""### Creating the model"""

# Hyperparameters to tune

batch_size = 256  # Reduced from 512
#gradient_accumulation_steps = 16  # Accumulate gradients over 4 steps
num_epochs = 12 # reduced from 15
batches_per_epoch = len(tokenized_train_data["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)

# Allow for use of multiple GPUs
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer,
                                            return_tensors='tf')

    # Turning dataset into TF dataset
    tf_train_dataset = tokenized_train_data["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "label"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator)

    tf_val_dataset = tokenized_val_data["val"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "label"],
    shuffle=False,
    batch_size=batch_size,
    collate_fn=data_collator)


print(tf_train_dataset)
print(tf_val_dataset)

# Using HuggingFace library to create optimizer
lr_scheduler = PolynomialDecay(
initial_learning_rate=5e-5, end_learning_rate=5e-7,
decay_steps=total_train_steps)

    # Loading the DistilBERT model and weights with a classification head
model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased",
                                                                     num_labels=len(affiliation_vocab))

    # Create the Adam optimizer within the strategy scope

optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler)


    #model.compile(optimizer=optimizer, # Pass 'adam' as a string
                  #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Explicitly define the loss function
     ##             metrics=['accuracy']) # Explicitly define the metrics

      # Loading the DistilBERT model and weights with a classification head
model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased",
                                                                 num_labels=len(affiliation_vocab))
model.compile(optimizer=optimizer,metrics=['accuracy'])

model.fit(tf_train_dataset, validation_data=tf_val_dataset, epochs=num_epochs)

tf_save_directory = f"{base_path}V2/002_Model/Result_model_lang"

# Saving the model, tokenizer, and affiliation (target) vocab
tokenizer.save_pretrained(tf_save_directory)

model.save_pretrained(tf_save_directory)
with open(f"{tf_save_directory}/vocab.pkl", "wb") as f:
    pickle.dump(affiliation_vocab, f)

print(model)

## comentar en script
!colab-convert /content/drive/MyDrive/openalex-institution-parsing/V2/002_Model/Notebooks/002b_language_model.ipynb /content/drive/MyDrive/openalex-institution-parsing/V2/002_Model/Notebooks/002b_language_model.py

print("Terminó")