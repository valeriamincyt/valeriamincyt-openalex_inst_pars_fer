# -*- coding: utf-8 -*-
"""001_get_training_data_SPARK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NbJ3f9c0NvruY-BYa9GKYg9pAxTeSMoz
"""

import pickle
#!pip install boto3
import boto3
#!pip install fsspec
import fsspec
import re
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#!pip install pyspark
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
#sc = spark.sparkContext         # Since Spark 2.0 'spark' is a
#SparkSession object that is by default created upfront and available
#in Spark shell, you need to explicitly create SparkSession object by
#using builder
spark = SparkSession.builder.getOrCreate()
#sc = SparkContext().getOrCreate()
sc = SparkContext._active_spark_context #devuelve la instancia existente
from pyspark.sql import SQLContext
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType, DoubleType, StructType, StructField
sqlContext = SQLContext(sc,spark)

import os

!pip install pyalex
import pyalex
from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders
from pyalex import config

import pandas as pd

"""**Cambiar path para correr desde otro lado**"""

#base_path = '/content/drive/MyDrive/openalex-institution-parsing/'
base_path = '../../../'

#base_save_path = "/content/drive/MyDrive/openalex-institution-parsing/V2/002_Model/"
base_save_path = f'{base_path}V2/002_Model/'

#iteration_save_path = "/content/drive/MyDrive/openalex-institution-parsing/V2/002_Model/"
iteration_save_path = base_save_path

"""### Getting all data (From saved OpenAlex DB snapshot)"""

#institutions = spark.read.parquet(f"{base_save_path}/OA_static_institutions_single_file.parquet") \
 #   .filter(F.col('ror_id')!='')

#institutions_df = pd.read_parquet("/content/drive/MyDrive/openalex-institution-parsing/Archivos/OA_static_institutions_single_file.parquet")
institutions_df = pd.read_parquet(f'{base_path}Crudos/OA_static_institutions_single_file.parquet')

### ALTERNATIVA DE STATIC INSTITUTIONS: BAJAR CON PYALEX PARTE DEL OBJETO INSTITUTIONS

# config.max_retries = 0
# config.retry_backoff_factor = 0.1
# config.retry_http_codes = [429, 500, 503]

# pager = Institutions().filter(country_code="AR").paginate(method="page",per_page=200)

# listaDeInstituciones = list()
# for page in pager:
#     print(len(page))
#     listaDeInstituciones += page

# print(len(listaDeInstituciones))
#institutions_df=pd.DataFrame(listaDeInstituciones)

institutions_df['affiliation_id'] = institutions_df['id'].apply(lambda x: x.split("/")[-1])
institutions_df['affiliation_id']=institutions_df['affiliation_id'].apply(lambda x: x.split("I")[-1])
institutions_df['ror_id'] = institutions_df['ror'].apply(lambda x: x.split("/")[-1])
print(institutions_df.head(n=2))
print(institutions_df.shape)

##STATIC AFFILIATION!
##utilizando el que estaba en drive
##affiliations = spark.read.parquet(f"{'/content/drive/MyDrive/openalex-institution-parsing/Archivos'}/static_affiliations.parquet")

#### ALTERNATIVA DE STATIC AFFILIATION: BAJAR DESDE OPENALEX CON PYALEX

pager = Works().filter(authorships={"institutions": {"country_code": "AR"}}).paginate(method="page",per_page=200)

listaDeWorks = list()
for page in pager:
    print(len(page))
    listaDeWorks += page

argentina_works=pd.DataFrame(listaDeWorks)

argentina_works['affiliations'] = argentina_works['authorships'].apply(lambda x: [i['affiliations'] for i in x] if isinstance(x,list) else [])
print(argentina_works['affiliations'].head())
print(argentina_works.shape)
print(argentina_works.head())

argentina_works['raw_affiliation_string'] = argentina_works['affiliations'].apply(lambda x: [d.get('raw_affiliation_string') for sublist in x for d in sublist if isinstance(sublist, list) and isinstance(d, dict)] if isinstance(x, list) else [])
argentina_works['institution_ids']        = argentina_works['affiliations'].apply(lambda x: [d.get('institution_ids') for sublist in x for d in sublist if isinstance(sublist, list) and isinstance(d, dict)] if isinstance(x, list) else [])
argentina_works=argentina_works.explode('institution_ids')
print(argentina_works.head(2))

argentina_works=argentina_works[['id','institution_ids','raw_affiliation_string']]
print(argentina_works.head(2))

# Ensure that 'institution_ids' and 'raw_affiliation_string' have the same number of elements for each row.

# Option 1: Zip the two columns, filter out unequal length entries, then explode
# Create a new column with zipped values
argentina_works['zipped_data'] = argentina_works.apply(
    lambda row: list(zip(row['institution_ids'], row['raw_affiliation_string']))
    if len(row['institution_ids']) == len(row['raw_affiliation_string']) else [], axis=1
)
# Filter out rows with empty zipped_data (unequal length entries)
argentina_works = argentina_works[argentina_works['zipped_data'].apply(len) > 0]

# Explode the zipped column
argentina_works1 = argentina_works.explode('zipped_data')

# Split the zipped data back into original columns
argentina_works1[['institution_ids', 'raw_affiliation_string']] = pd.DataFrame(
    argentina_works1['zipped_data'].tolist(), index=argentina_works1.index
)

print(argentina_works1.head(2))

argentina_works1['works_id'] = argentina_works1['id'].apply(lambda x: x.split("/")[-1])
#argentina_works1['original_affiliation']=argentina_works1['raw_affiliation_string'].apply(lambda x: x.split("']")[0])
argentina_works1['institution_ids'] = argentina_works1['institution_ids'].apply(lambda x: x.split("/")[-1])
argentina_works1['institution_ids'] =argentina_works1['institution_ids'] .apply(lambda x: x.split("I")[-1])
argentina_works1['affiliation_id']=argentina_works1['institution_ids']
argentina_works1['original_affiliation']=argentina_works1['raw_affiliation_string']

print(argentina_works1.shape)
affiliations=argentina_works1
print(affiliations.head(2))



##agregado para armar dataset de prueba en testeo
affiliation_muestra_para_testeo=affiliations.sample(frac=0.1)
affiliation_muestra_para_testeo.shape
affiliation_muestra_para_testeo.head()
affiliation_muestra_para_testeo=affiliation_muestra_para_testeo[['institution_ids','raw_affiliation_string','works_id']]
affiliation_muestra_para_testeo=affiliation_muestra_para_testeo.rename(columns={'institution_ids':'labels', 'works_id': 'paper_id', 'raw_affiliation_string': 'affiliation_string'})
print(affiliation_muestra_para_testeo.head())
print(affiliation_muestra_para_testeo.shape)
affiliation_muestra_para_testeo.to_csv(f"{base_path}V2/002_Model/affiliations_para_testeo.csv")

##nos quedamos con estas observaciones para affiliations
affiliations1=affiliations-affiliation_muestra_para_testeo

print(affiliations1.shape)

os.chdir(f"{base_path}V2/002_Model/")

# Now you can safely write the parquet file
affiliations1.to_parquet('affiliations.parquet')

# Assuming 'spark' is your SparkSession object
affiliations = spark.read.parquet(f"{base_path}V2/002_Model/affiliations.parquet")

print(affiliations.head())

"""#### Getting ROR aff strings"""

dedup_affs = affiliations.select(F.trim(F.col('original_affiliation')).alias('original_affiliation'), 'affiliation_id')\
.filter(F.col('original_affiliation').isNotNull())\
.filter(F.col('original_affiliation')!='')\
.withColumn('aff_len', F.length(F.col('original_affiliation')))\
.filter(F.col('aff_len')>2)\
.groupby(['original_affiliation','affiliation_id']) \
.agg(F.count(F.col('affiliation_id')).alias('aff_string_counts'))

dedup_affs.cache().count()

dedup_affs.head(n=3)

ror_data = spark.read.parquet(f"{base_path}V2/001_Exploration/ror_strings.parquet") \
.select('original_affiliation','affiliation_id')

ror_data.cache().count()

ror_data.head(n=2)

"""### Gathering training data

Since we are looking at all institutions, we need to up-sample the institutions that don't have many affiliation strings and down-sample the institutions that have large numbers of strings. There was a balance here that needed to be acheived. The more samples that are taken for each institution, the more overall training data we will have and the longer our model will take to train.

However, more samples also means more ways of an institution showing up in an affiliation string.

The number of samples was set to 50 as it was determined this was a good optimization point based on affiliation string count distribution and time it would take to train the model.

However, unlike in V1 where we tried to keep all institutions at 50, for V2 we gave additional samples for institutions with more strings available. Specifically, we allowed those institutions to have up to 25 additional strings, for a total of 75.
"""

num_samples_to_get = 50

w1 = Window.partitionBy('affiliation_id')

filled_affiliations = dedup_affs \
    .join(ror_data.select('affiliation_id'), how='inner', on='affiliation_id') \
    .select('original_affiliation','affiliation_id') \
    .union(ror_data.select('original_affiliation','affiliation_id')) \
    .filter(~F.col('affiliation_id').isNull()) \
    .dropDuplicates() \
    .withColumn('random_prob', F.rand(seed=20)) \
    .withColumn('id_count', F.count(F.col('affiliation_id')).over(w1)) \
    .withColumn('scaled_count', F.lit(1)-((F.col('id_count') - F.lit(num_samples_to_get))/(F.lit(3500000) - F.lit(num_samples_to_get)))) \
    .withColumn('final_prob', F.col('random_prob')*F.col('scaled_count'))

filled_affiliations.select('affiliation_id').distinct().count()

filled_affiliations.head(n=2)



less_than = filled_affiliations.dropDuplicates(subset=['affiliation_id']).filter(F.col('id_count') < num_samples_to_get).toPandas()

less_than['affiliation_id'] = less_than['affiliation_id'].astype('str')
less_than['affiliation_id'] = less_than['affiliation_id'].apply(lambda x: x.split("/")[-1])

print(less_than.shape)
print(less_than.head(n=2))

temp_df_list = []
for aff_id in less_than['affiliation_id'].unique():
    temp_df = less_than[less_than['affiliation_id']==aff_id].copy()
    help_df = temp_df.sample(num_samples_to_get - temp_df.shape[0], replace=True)
    temp_df_list.append(pd.concat([temp_df, help_df], axis=0))
less_than_df = pd.concat(temp_df_list, axis=0)

# only install fsspec and s3fs
less_than_df[['original_affiliation', 'affiliation_id']].to_parquet(f"{iteration_save_path}lower_than_{num_samples_to_get}.parquet")
w1 = Window.partitionBy('affiliation_id').orderBy('random_prob')

more_than = filled_affiliations.filter(F.col('id_count') >= num_samples_to_get) \
.withColumn('row_number', F.row_number().over(w1)) \
.filter(F.col('row_number') <= num_samples_to_get+25)

more_than.cache().count()


more_than=more_than.toPandas()

more_than['affiliation_id'] = more_than['affiliation_id'].astype('str')
more_than['affiliation_id'] = more_than['affiliation_id'].apply(lambda x: x.split("/")[-1])
more_than[['original_affiliation', 'affiliation_id']].to_parquet(f"{iteration_save_path}more_than_{num_samples_to_get}.parquet")


#more_than.select('original_affiliation', 'affiliation_id') \
#.coalesce(1).write.mode('overwrite').parquet(f"{iteration_save_path}more_than_{num_samples_to_get}")



print(less_than.sample(5))

less_than_df.shape

print(more_than.head(5))

more_than.shape

##comentar en script
!colab-convert /content/drive/MyDrive/openalex-institution-parsing/V2/002_Model/Notebooks/001_get_training_data_SPARK.ipynb /content/drive/MyDrive/openalex-institution-parsing/V2/002_Model/Notebooks/001_get_training_data_SPARK.py

print("Terminó")